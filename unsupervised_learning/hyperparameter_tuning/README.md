# Hyperparameter Tuning Project

## Overview
This project implements **Bayesian Optimization** for hyperparameter tuning, from scratch and with GPyOpt.

## Project Structure

### Exercises 0-5: Gaussian Process Implementation
Building blocks for Bayesian Optimization:

- **0-gp.py**: `GaussianProcess` class with RBF kernel
- **1-gp.py**: Add `predict()` method for mean and variance estimation
- **2-gp.py**: Add `update()` method to add new samples
- **3-bayes_opt.py**: `BayesianOptimization` class initialization
- **4-bayes_opt.py**: `acquisition()` method using Expected Improvement (EI)
- **5-bayes_opt.py**: `optimize()` method for iterative optimization

### Exercise 6: Real-World Application with GPyOpt
**6-bayes_opt.py**: Optimize a neural network on the Iris dataset

## Exercise 6: Detailed Explanation

### What it does:
Automatically finds the **best hyperparameters** for a neural network using Bayesian Optimization.

### Hyperparameters tuned (5 total):
1. **Learning Rate** (0.0001 - 0.1): controls training speed
2. **Units** (16, 32, 64, 128): neurons in first layer
3. **Dropout Rate** (0.0 - 0.5): regularization to prevent overfitting
4. **L2 Regularization** (0.0 - 0.01): weight penalty
5. **Batch Size** (8, 16, 32, 64): samples per training step

### Optimization process:
1. **Initial sampling** (5 iterations): random hyperparameter combinations
2. **Iterative optimization** (25 iterations): uses Expected Improvement to choose promising hyperparameters
3. **Early stopping**: halts training if validation loss stops improving
4. **Checkpoint saving**: saves best model weights for each configuration
5. **Convergence tracking**: monitors loss improvement over iterations

### Outputs:
- **bayes_opt.txt**: Optimization report with best parameters and metrics
- **convergence.png**: Graph showing loss improvement over iterations
- **best_model_*.h5**: Saved model weights from best iterations

### Key results:
- Best validation loss found automatically
- Hyperparameter values that achieve this loss
- Total optimization time and iteration count
- Visual proof of convergence

## How to Run

```bash
# Install dependencies
pip install tensorflow GPyOpt scikit-learn matplotlib numpy scipy

# Run optimization
./6-bayes_opt.py

# Check results
cat bayes_opt.txt
open convergence.png
ls -la best_model_*.h5
```

## Why Bayesian Optimization?
Traditional hyperparameter tuning (grid search, random search) requires many evaluations. Bayesian Optimization uses a **Gaussian Process** to:
- Learn from previous evaluations
- Predict promising regions
- Balance exploration vs exploitation
- Find good hyperparameters **faster**

## ðŸ“Š BONUS: Understanding the Convergence Plot

The `convergence.png` file shows two graphs generated by GPyOpt that reveal the optimization strategy:

### Left Graph: Distance Between Successive Hyperparameter Choices

**Axes:**
- **X-axis**: Iteration number
- **Y-axis**: Distance d(x_n, x_{n-1}) between hyperparameters at iteration n and n-1

**What it means:**
- **High curve** â†’ Algorithm proposes very different configurations (EXPLORATION)
  - Far jumps in hyperparameter space
  - Testing new, untested regions
- **Curve near 0** â†’ Two successive iterations test very similar points (EXPLOITATION)
  - Small steps around a promising zone
  - Fine-tuning around a good solution

**Pattern you'll see:**
An alternating pattern of large jumps and small steps, showing the classic **exploration/exploitation trade-off** of Expected Improvement:
- Early iterations: big jumps to explore different regions
- Later iterations: small steps to refine around good candidates

### Right Graph: Best Loss Over Time

**Axes:**
- **X-axis**: Iteration number
- **Y-axis**: Best validation loss found so far (lowest loss up to that iteration)

**What it means:**
- This is a **descending staircase curve**
- Each step down = new best solution found
- Flat sections = current best not beaten by recent iterations

**Typical behavior:**
1. **Start (iterations 1-10)**: Steep drops, big improvements
2. **Middle (iterations 10-20)**: Medium improvements, frequent new records
3. **End (iterations 20-30)**: Small steps, rare improvements
   - Gains become marginal
   - Algorithm has converged to a good region
   - Further exploration unlikely to find better solutions

**Why this pattern?**
The algorithm quickly finds a decent region, then spends iterations refining that region. Once diminishing returns set in, the curve flattensâ€”this is **convergence**.

### Interpreting Success
âœ… **Good optimization** shows:
- Steep initial descent in right graph
- Mix of exploration and exploitation in left graph
- Convergence to a stable loss level
- Total time reasonable (< 5 minutes for Iris dataset)

## Technologies Used
- **TensorFlow/Keras**: Neural network training
- **GPyOpt**: Bayesian Optimization framework
- **Scikit-learn**: Data preprocessing and datasets
- **NumPy/SciPy**: Numerical computation
- **Matplotlib**: Visualization

